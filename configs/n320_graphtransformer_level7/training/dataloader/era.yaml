prefetch_factor: 2
pin_memory: True

# ============
# read_group_size:
#   Form subgroups of model comm groups that read data together.
#   Each reader in the group only reads 1/read_group_size of the data
#   which is then all-gathered between the group.
#   This can reduce CPU memory usage as well as increase dataloader throughput.
#   The number of GPUs per model must be divisible by read_group_size.
#   To disable, set to 1.
# ============
read_group_size: ${hardware.num_gpus_per_model}

num_workers:
  training: 6
  validation: 8
  test: 8
  predict: 8
batch_size:
  training: 1
  validation: 1
  test: 4
  predict: 4

# ============
# Default effective batch_size for training is 16
# For the o96 resolution, default per-gpu batch_size is 2 (8 gpus required)
# The global lr is calculated as:
# global_lr = local_lr * num_gpus_per_node * num_nodes / gpus_per_model
# Assuming a constant effective batch_size, any change in the per_gpu batch_size
# should come with a rescaling of the local_lr to keep a constant global_lr
# ============

# runs only N training batches [N = integer | null]
# if null then we run through all the batches
limit_batches:
  training: null
  validation: null
  test: 20
  predict: 20

# set a custom mask for grid points.
# Useful for LAM (dropping unconnected nodes from forcing dataset)
grid_indices:
  _target_: anemoi.training.data.grid_indices.FullGrid
  nodes_name: ${graph.data}

# ============
# Dataloader definitions
# These follow the anemoi-datasets patterns
# You can make these as complicated for merging as you like
# See https://anemoi-datasets.readthedocs.io
# ============

# sfc: '10u', '10v', '2d', '2t', 'msl', 'sp', ‘skt’, ‘tp’
# pl: 'q', 't', 'u', 'v', ‘z’
# pl.levels: 50, 100, 150, 200, 250, 300, 400, 500, 700, 850, 925, 1000  # drop 600
# forcing: lsm, insolation, cos_julian_day, cos_latitude, cos_local_time, cos_longitude, sin_julian_day, sin_latitude, sin_local_time, sin_longitude

dataset: 
  dataset: ${hardware.files.dataset}
  select: ['10u', '10v', '2d', '2t', 'msl', 'sp', 'skt', 'z', 'tp', 'q_50', 'q_100', 'q_150', 'q_200', 'q_250', 'q_300', 'q_400', 'q_500', 'q_700', 'q_850', 'q_925', 'q_1000', 't_50', 't_100', 't_150', 't_200', 't_250', 't_300', 't_400', 't_500', 't_700', 't_850', 't_925', 't_1000', 'u_50', 'u_100', 'u_150', 'u_200', 'u_250', 'u_300', 'u_400', 'u_500', 'u_700', 'u_850', 'u_925', 'u_1000', 'v_50', 'v_100', 'v_150', 'v_200', 'v_250', 'v_300', 'v_400', 'v_500', 'v_700', 'v_850', 'v_925', 'v_1000', 'z_50', 'z_100', 'z_150', 'z_200', 'z_250', 'z_300', 'z_400', 'z_500', 'z_700', 'z_850', 'z_925', 'z_1000', lsm, insolation, cos_julian_day, cos_latitude, cos_local_time, cos_longitude, sin_julian_day, sin_latitude, sin_local_time, sin_longitude]


training:
  dataset: ${dataloader.dataset}
  start: null
  end: 2022
  frequency: ${data.frequency}
  drop:  []

validation_rollout: 1 # number of rollouts to use for validation, must be equal or greater than rollout expected by callbacks

validation:
  dataset: ${dataloader.dataset}
  start: 2023
  end: null
  frequency: ${data.frequency}
  drop:  []

test:
  dataset: ${dataloader.dataset}
  start: 2023
  end: null
  frequency: ${data.frequency}
